{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julihocc/edx-ibm-DL0120EN/blob/main/202-transfer-learning/M02L02-Lab-Transfer-Learning-Implementation-v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8afe0c15-9bcd-4994-99c6-920fd5868b27"
      },
      "source": [
        "<p style=\"text-align:center\">\n",
        "    <a href=\"https://skills.network\" target=\"_blank\">\n",
        "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
        "    </a>\n",
        "</p>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "917c0a3e-3a81-40bd-ac3f-a548ec8fbb34"
      },
      "source": [
        "# Lab: Transfer Learning Implementation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecadeb4b-7fd4-4590-b94c-8a322c0707bc"
      },
      "source": [
        "##### Estimated time needed:  30 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25da3076-e822-4984-afc3-bb0b5dd23b16"
      },
      "source": [
        "In this lab, you will learn to implement transfer learning using a pre-trained model in Keras.\n",
        "\n",
        "#### Learning objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "\n",
        " - Import necessary libraries and load the dataset.\n",
        " - Load a pre-trained model, VGG16, excluding the top layers.\n",
        " - Add new layers on top of the base model and compile the model.\n",
        " - Train the model on the new dataset.\n",
        " - Unfreeze some of the layers of the pre-trained model and fine-tune them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1f3ed27-f588-491b-9997-dbbeb70779ed"
      },
      "source": [
        "### Step-by-Step Guide:\n",
        "\n",
        "#### Step 1: Setup the Environment\n",
        "\n",
        "Before we start, make sure to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "769b7642-9ffa-496f-a22b-900f7ebcbad1"
      },
      "outputs": [],
      "source": [
        "# !pip install tensorflow==2.16.2 matplotlib==3.9.1\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "135bbd47-c7b2-4e71-968e-200fbe7b8cc9"
      },
      "source": [
        "##### Explanation:\n",
        "- `tensorflow` is the main library for machine learning in Python.\n",
        "- `Sequential` is used to create a model with a linear stack of layers.\n",
        "- `Dense` and `Flatten` are types of layers that we will use in our model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5fa4e01-165a-4387-aace-da7fd56ce78c"
      },
      "source": [
        "#### Step 2: Load Pre-trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3f4e5fb4-7b2b-4487-8cf7-94a2a69aefc4",
        "outputId": "c6a947e2-7898-4b64-e141-9c9766df43a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Load the VGG16 model pre-trained on ImageNet\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0b4ac35-5359-4c74-b7bf-8d4b72007fbb"
      },
      "source": [
        "#### Step 3: Create and Compile the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9ab87715-f69e-4889-a306-40fe809988b0"
      },
      "outputs": [],
      "source": [
        "# Create a new model and add the base model and new layers\n",
        "model = Sequential([\n",
        "    base_model,\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Change to the number of classes you have\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75719b73-022a-479d-8c48-6389a76442a2"
      },
      "source": [
        "### **Create Placeholder Images**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9fc34894-ddc4-47f0-a325-34709db0b39e",
        "outputId": "d6a0603e-74b4-4f60-ec61-bf9802f7d439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample images created in 'sample_data/'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs('sample_data/class_a', exist_ok=True)\n",
        "os.makedirs('sample_data/class_b', exist_ok=True)\n",
        "\n",
        "# Create 10 sample images for each class\n",
        "for i in range(10):\n",
        "    # Create a blank white image for class_a\n",
        "    img = Image.fromarray(np.ones((224, 224, 3), dtype=np.uint8) * 255)\n",
        "    img.save(f'sample_data/class_a/img_{i}.jpg')\n",
        "\n",
        "    # Create a blank black image for class_b\n",
        "    img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
        "    img.save(f'sample_data/class_b/img_{i}.jpg')\n",
        "\n",
        "print(\"Sample images created in 'sample_data/'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c0bb8c1-1073-414e-bae2-8ba3ad029c08"
      },
      "source": [
        "#### Step 4: Train the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "65d62928-699b-465f-a487-e3ab61284fb1",
        "outputId": "57dcfc75-c481-4b3f-e45e-842618cb5831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 13s/step - accuracy: 0.5000 - loss: 0.7259\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.5000 - loss: 6.3126\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.5000 - loss: 3.0702\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 276ms/step - accuracy: 0.5000 - loss: 0.5427\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.5000 - loss: 0.6925\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 0.5000 - loss: 0.6922\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step - accuracy: 0.5000 - loss: 0.6919\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.5000 - loss: 0.6915\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step - accuracy: 0.5000 - loss: 0.6910\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 134ms/step - accuracy: 0.5000 - loss: 0.6905\n"
          ]
        }
      ],
      "source": [
        "# Load and preprocess the dataset\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'sample_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Verify if the generator has loaded images correctly\n",
        "print(f\"Found {train_generator.samples} images belonging to {train_generator.num_classes} classes.\")\n",
        "\n",
        "# Train the model\n",
        "if train_generator.samples > 0:\n",
        "    model.fit(train_generator, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef0924e6-440a-4f04-b917-24f16c7bc643"
      },
      "source": [
        "#### Step 5: Fine-Tune the Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8c54f44f-a7e4-4f31-ae94-c02d8c809fcc",
        "outputId": "0751bceb-4fd6-4fbf-f7b8-02bb3cdbae18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - accuracy: 0.5000 - loss: 0.6899\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step - accuracy: 0.5000 - loss: 0.6827\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step - accuracy: 0.5000 - loss: 3.0384\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step - accuracy: 0.5000 - loss: 1.4549\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 302ms/step - accuracy: 0.5000 - loss: 0.6947\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.5000 - loss: 0.6735\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.5000 - loss: 0.6300\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.5000 - loss: 0.6037\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 306ms/step - accuracy: 0.5000 - loss: 0.9955\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step - accuracy: 0.5000 - loss: 0.5816\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f1f1356fee0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Unfreeze the top layers of the base model\n",
        "\n",
        "for layer in base_model.layers[-4:]:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Compile the model again\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model again\n",
        "model.fit(train_generator, epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9d5e7f8-93b3-466e-aecc-2e51d8a07c35"
      },
      "source": [
        "### Exercises\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "200ce10c-d5da-4972-8c6e-8cfaf7056c0a"
      },
      "source": [
        "#### Exercise 1: Visualize Training and Validation Loss\n",
        "\n",
        "**Objective:** Plot the training and validation loss to observe the learning process of the model.\n",
        "\n",
        "**Instructions:**\n",
        "1. Modify the training code to include validation data.\n",
        "2. Plot the training and validation loss for each epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "efe25af5-af7e-4398-8474-eb91167597ce"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf1e5eea-ce2d-4090-9b23-186f3d5ed697"
      },
      "source": [
        "<details>\n",
        "<summary>Click here for solution</summary> </br>\n",
        "\n",
        "```python\n",
        "# Modify data generator to include validation data\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'sample_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    'sample_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Train the model with validation data\n",
        "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Plot training and validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "490266d9-6dd7-4944-81ce-4615a8cab5f8"
      },
      "source": [
        "#### Exercise 2: Experiment with Different Optimizers\n",
        "\n",
        "**Objective:** Experiment with different optimizers and observe their impact on model performance.\n",
        "\n",
        "**Instructions:**\n",
        "1. Change the optimizer from `adam` to `sgd` and `rmsprop`.\n",
        "2. Retrain the model with each optimizer and compare the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0ec077e3-cac6-4f43-955b-8c84ef791e58"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db02a223-c9bb-4649-b397-50ae12d92d09"
      },
      "source": [
        "<details>\n",
        "<summary>Click here for solution</summary> </br>\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import clone_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Function to reset the model weights\n",
        "def reset_model(model):\n",
        "    # Clone the model to reset weights\n",
        "    model_clone = clone_model(model)\n",
        "    model_clone.set_weights(model.get_weights())\n",
        "    return model_clone\n",
        "\n",
        "# Prepare to reset the model for each optimizer test\n",
        "initial_model = reset_model(model)  # Assume 'model' is the initial compiled model\n",
        "\n",
        "# Experiment with SGD optimizer\n",
        "sgd_model = reset_model(initial_model)  # Reset model\n",
        "sgd_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_sgd = sgd_model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Plot training and validation accuracy for SGD\n",
        "plt.plot(history_sgd.history['accuracy'], label='Training Accuracy SGD')\n",
        "plt.plot(history_sgd.history['val_accuracy'], label='Validation Accuracy SGD')\n",
        "plt.title('Training and Validation Accuracy with SGD')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Experiment with RMSprop optimizer\n",
        "rmsprop_model = reset_model(initial_model)  # Reset model\n",
        "rmsprop_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history_rmsprop = rmsprop_model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
        "\n",
        "# Plot training and validation accuracy for RMSprop\n",
        "plt.plot(history_rmsprop.history['accuracy'], label='Training Accuracy RMSprop')\n",
        "plt.plot(history_rmsprop.history['val_accuracy'], label='Validation Accuracy RMSprop')\n",
        "plt.title('Training and Validation Accuracy with RMSprop')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "731c6f21-35e4-4281-9116-0b302c30b000"
      },
      "source": [
        "#### Exercise 3: Evaluate the Model on a Test Set\n",
        "\n",
        "**Objective:** Evaluate the fine-tuned model on an unseen test set to assess its generalization performance.\n",
        "\n",
        "**Instructions:**\n",
        "1. Load a separate test set.\n",
        "2. Evaluate the model on this test set and report the accuracy and loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2e9580ec-b3f2-4f51-a668-c227406ac16b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea02abc8-07d8-4be1-962f-54d29d692a3a"
      },
      "source": [
        "<details>\n",
        "<summary>Click here for solution</summary> </br>\n",
        "\n",
        "```python\n",
        "# Load and preprocess the test dataset\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    'sample_data',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# Evaluate the fine-tuned model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
        "print(f'Test Loss: {test_loss:.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb74232-a593-45cb-9913-7b3553553dfc"
      },
      "source": [
        "### Summary\n",
        "\n",
        "By completing these exercises, students will:\n",
        "\n",
        "1. Visualize the training and validation loss to gain insights into the training process.\n",
        "2. Experiment with different optimizers to understand their impact on model performance.\n",
        "3. Evaluate the fine-tuned model on an unseen test set to assess its generalization capability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a3acd0a-6ab4-4389-8ed2-b5381b67a970"
      },
      "source": [
        "#### Conclusion\n",
        "\n",
        "Congratulations! In this lab, you have successfully implemented transfer learning using a pre-trained model in Keras. This lab exercise demonstrated how to train and fine-tune the model by unfreezing some of the layers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad8a425e-4959-4b3b-ae47-9b2445326c25"
      },
      "source": [
        "Copyright © IBM Corporation. All rights reserved.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "prev_pub_hash": "46890cfd422ab815a33a7c99b85ad21a549fbfa26e2bfd3ec07a5686815da9bc",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}